{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNeural Transport\n================\n\nThis example illustrates how to use a trained AutoBNAFNormal autoguide to transform a posterior to a\nGaussian-like one. The transform will be used to get better mixing rate for NUTS sampler.\n\n**References:**\n\n    1. Hoffman, M. et al. (2019), \"NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport\",\n       (https://arxiv.org/abs/1903.03704)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nfrom functools import partial\nimport os\n\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom jax import lax, random, vmap\nimport jax.numpy as np\nfrom jax.tree_util import tree_map\n\nimport numpyro\nfrom numpyro import optim\nfrom numpyro.contrib.autoguide import AutoContinuousELBO, AutoBNAFNormal\nfrom numpyro.diagnostics import print_summary\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints\nfrom numpyro.infer import MCMC, NUTS, SVI\nfrom numpyro.infer.util import initialize_model, transformed_potential_energy\n\n\n# XXX: upstream logsumexp throws NaN under fast-math mode + MCMC's progress_bar=True\ndef logsumexp(x, axis=0):\n    return np.log(np.sum(np.exp(x), axis=axis))\n\n\nclass DualMoonDistribution(dist.Distribution):\n    support = constraints.real_vector\n\n    def __init__(self):\n        super(DualMoonDistribution, self).__init__(event_shape=(2,))\n\n    def sample(self, key, sample_shape=()):\n        # it is enough to return an arbitrary sample with correct shape\n        return np.zeros(sample_shape + self.event_shape)\n\n    def log_prob(self, x):\n        term1 = 0.5 * ((np.linalg.norm(x, axis=-1) - 2) / 0.4) ** 2\n        term2 = -0.5 * ((x[..., :1] + np.array([-2., 2.])) / 0.6) ** 2\n        pe = term1 - logsumexp(term2, axis=-1)\n        return -pe\n\n\ndef dual_moon_model():\n    numpyro.sample('x', DualMoonDistribution())\n\n\ndef main(args):\n    print(\"Start vanilla HMC...\")\n    nuts_kernel = NUTS(dual_moon_model)\n    mcmc = MCMC(nuts_kernel, args.num_warmup, args.num_samples,\n                progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True)\n    mcmc.run(random.PRNGKey(0))\n    mcmc.print_summary()\n    vanilla_samples = mcmc.get_samples()['x'].copy()\n\n    guide = AutoBNAFNormal(dual_moon_model, hidden_factors=[args.hidden_factor, args.hidden_factor])\n    svi = SVI(dual_moon_model, guide, optim.Adam(0.003), AutoContinuousELBO())\n    svi_state = svi.init(random.PRNGKey(1))\n\n    print(\"Start training guide...\")\n    last_state, losses = lax.scan(lambda state, i: svi.update(state), svi_state, np.zeros(args.num_iters))\n    params = svi.get_params(last_state)\n    print(\"Finish training guide. Extract samples...\")\n    guide_samples = guide.sample_posterior(random.PRNGKey(0), params,\n                                           sample_shape=(args.num_samples,))['x'].copy()\n\n    transform = guide.get_transform(params)\n    _, potential_fn, constrain_fn = initialize_model(random.PRNGKey(2), dual_moon_model)\n    transformed_potential_fn = partial(transformed_potential_energy, potential_fn, transform)\n    transformed_constrain_fn = lambda x: constrain_fn(transform(x))  # noqa: E731\n\n    print(\"\\nStart NeuTra HMC...\")\n    nuts_kernel = NUTS(potential_fn=transformed_potential_fn)\n    mcmc = MCMC(nuts_kernel, args.num_warmup, args.num_samples,\n                progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True)\n    init_params = np.zeros(guide.latent_size)\n    mcmc.run(random.PRNGKey(3), init_params=init_params)\n    mcmc.print_summary()\n    zs = mcmc.get_samples()\n    print(\"Transform samples into unwarped space...\")\n    samples = vmap(transformed_constrain_fn)(zs)\n    print_summary(tree_map(lambda x: x[None, ...], samples))\n    samples = samples['x'].copy()\n\n    # make plots\n\n    # guide samples (for plotting)\n    guide_base_samples = dist.Normal(np.zeros(2), 1.).sample(random.PRNGKey(4), (1000,))\n    guide_trans_samples = vmap(transformed_constrain_fn)(guide_base_samples)['x']\n\n    x1 = np.linspace(-3, 3, 100)\n    x2 = np.linspace(-3, 3, 100)\n    X1, X2 = np.meshgrid(x1, x2)\n    P = np.exp(DualMoonDistribution().log_prob(np.stack([X1, X2], axis=-1)))\n\n    fig = plt.figure(figsize=(12, 8), constrained_layout=True)\n    gs = GridSpec(2, 3, figure=fig)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[1, 0])\n    ax3 = fig.add_subplot(gs[0, 1])\n    ax4 = fig.add_subplot(gs[1, 1])\n    ax5 = fig.add_subplot(gs[0, 2])\n    ax6 = fig.add_subplot(gs[1, 2])\n\n    ax1.plot(losses[1000:])\n    ax1.set_title('Autoguide training loss\\n(after 1000 steps)')\n\n    ax2.contourf(X1, X2, P, cmap='OrRd')\n    sns.kdeplot(guide_samples[:, 0], guide_samples[:, 1], n_levels=30, ax=ax2)\n    ax2.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel='x0', ylabel='x1', title='Posterior using\\nAutoBNAFNormal guide')\n\n    sns.scatterplot(guide_base_samples[:, 0], guide_base_samples[:, 1], ax=ax3,\n                    hue=guide_trans_samples[:, 0] < 0.)\n    ax3.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel='x0', ylabel='x1', title='AutoBNAFNormal base samples\\n(True=left moon; False=right moon)')\n\n    ax4.contourf(X1, X2, P, cmap='OrRd')\n    sns.kdeplot(vanilla_samples[:, 0], vanilla_samples[:, 1], n_levels=30, ax=ax4)\n    ax4.plot(vanilla_samples[-50:, 0], vanilla_samples[-50:, 1], 'bo-', alpha=0.5)\n    ax4.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel='x0', ylabel='x1', title='Posterior using\\nvanilla HMC sampler')\n\n    sns.scatterplot(zs[:, 0], zs[:, 1], ax=ax5, hue=samples[:, 0] < 0.,\n                    s=30, alpha=0.5, edgecolor=\"none\")\n    ax5.set(xlim=[-5, 5], ylim=[-5, 5],\n            xlabel='x0', ylabel='x1', title='Samples from the\\nwarped posterior - p(z)')\n\n    ax6.contourf(X1, X2, P, cmap='OrRd')\n    sns.kdeplot(samples[:, 0], samples[:, 1], n_levels=30, ax=ax6)\n    ax6.plot(samples[-50:, 0], samples[-50:, 1], 'bo-', alpha=0.2)\n    ax6.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel='x0', ylabel='x1', title='Posterior using\\nNeuTra HMC sampler')\n\n    plt.savefig(\"neutra.pdf\")\n\n\nif __name__ == \"__main__\":\n    assert numpyro.__version__.startswith('0.2.4')\n    parser = argparse.ArgumentParser(description=\"NeuTra HMC\")\n    parser.add_argument('-n', '--num-samples', nargs='?', default=4000, type=int)\n    parser.add_argument('--num-warmup', nargs='?', default=1000, type=int)\n    parser.add_argument('--hidden-factor', nargs='?', default=8, type=int)\n    parser.add_argument('--num-iters', nargs='?', default=10000, type=int)\n    parser.add_argument('--device', default='cpu', type=str, help='use \"cpu\" or \"gpu\".')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n\n    main(args)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}