

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Bayesian Optimization &mdash; Pyro Tutorials 0.2.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Pyro Tutorials 0.2.0 documentation" href="index.html"/>
        <link rel="prev" title="Gaussian Processes" href="gp.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.2.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Models in Pyro: From Primitive Distributions to Stochastic Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Inference in Pyro: From Stochastic Functions to Marginal Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro 0.2</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">Bayesian Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Define-a-problem">Define a problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Set-a-Gaussian-Process-prior">Set a Gaussian Process prior</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Define-an-acquisition-function">Define an acquisition function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Find-">Find <span class="math notranslate">\(\{ x_n \}\)</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Bayesian Optimization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/bo.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Bayesian-Optimization">
<h1>Bayesian Optimization<a class="headerlink" href="#Bayesian-Optimization" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian
optimization</a> is
a powerful strategy to find extrema of objective functions which are
costly to evaluate, not easy to have access to their derivatives, and
possibly containing noise. It is a machenism lying behind <a class="reference external" href="https://en.wikipedia.org/wiki/Automated_machine_learning">automated
machine
learning</a>
toolboxes such as
<a class="reference external" href="https://automl.github.io/auto-sklearn/stable/">auto-sklearn</a>,
<a class="reference external" href="http://www.cs.ubc.ca/labs/beta/Projects/autoweka/">auto-weka</a>,
<a class="reference external" href="https://scikit-optimize.github.io/">scikit-optimize</a> to select best
hyperparameters for machine learning models. As mentioned in the review
reference [2], Bayesian optimization is used for a wide range of other
applications such as interactive user-interfaces, robotics,
environmental monitoring, information extraction, combinatorial
optimisation, sensor networks, adaptive Monte Carlo, experimental
design, reinforcement learning. The innovation <a class="reference external" href="https://en.wikipedia.org/wiki/AlphaGo_Zero">AlphaGo
Zero</a> also uses Bayesian
optimization to tune its tree search’s hyperparameters.</p>
<p>Assume that we have an optimization problem</p>
<div class="math notranslate">
\[x^* = \text{arg}\min \ f(x),\]</div>
<p>where <span class="math notranslate">\(f\)</span> is an unknown objective function. To solve it, we will
construct a sequence of points <span class="math notranslate">\(\{x_n\}\)</span> which converges to
<span class="math notranslate">\(x^*\)</span>. The Bayesian optimization strategy works as follows:</p>
<ol class="arabic">
<li><p class="first">Place a prior on the possibilities of the objective function. The
model will be updated each time we evaluate more data to form a
posterior distribution over the objective function.</p>
</li>
<li><p class="first">Derive from the posterior an “acquisition” function <span class="math notranslate">\(\alpha(x)\)</span>
which is easy to evaluate and get derivatives (hence optimizing
<span class="math notranslate">\(\alpha(x)\)</span> is easy).</p>
</li>
<li><p class="first">Repeat until convergence:</p>
<ul>
<li><p class="first">Use the acquisition function to derive the next query point
according to</p>
<div class="math notranslate">
\[x_{n+1} = \text{arg}\min \ \alpha(x).\]</div>
</li>
<li><p class="first">Evaluate <span class="math notranslate">\(f(x_{n+1})\)</span> and update the posterior.</p>
</li>
</ul>
</li>
</ol>
<p>The acquisition function should utilize the uncertainty of the posterior
to encourage exploring more information about <span class="math notranslate">\(f\)</span>. After
evaluating a new data point, the model will be updated and the derived
acquisition function will exploit it to give the next candidate for the
minimum point of <span class="math notranslate">\(f\)</span>. After a number of steps, it is likely that
<span class="math notranslate">\(\{x_n\}\)</span> will converge to <span class="math notranslate">\(x^*\)</span>.</p>
<p>In this tutorial, we will see how to use the <a class="reference external" href="http://docs.pyro.ai/en/dev/contrib.gp.html">Gaussian Process
module</a> in Pyro to do
Bayesian optimization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">matplotlib.gridspec</span> <span class="kn">as</span> <span class="nn">gridspec</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.autograd</span> <span class="kn">as</span> <span class="nn">autograd</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">constraints</span><span class="p">,</span> <span class="n">transform_to</span>

<span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">import</span> <span class="nn">pyro.contrib.gp</span> <span class="kn">as</span> <span class="nn">gp</span>

<span class="n">pyro</span><span class="o">.</span><span class="n">enable_validation</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># can help with debugging</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">set_rng_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Define-a-problem">
<h2>Define a problem<a class="headerlink" href="#Define-a-problem" title="Permalink to this headline">¶</a></h2>
<p>For the purpose of demonstration, the objective function we are going to
solve is the <a class="reference external" href="https://www.sfu.ca/~ssurjano/forretal08.html">Forrester et al. (2008)
function</a>:</p>
<div class="math notranslate">
\[f(x) = (6x-2)^2 \sin(12x-4), \quad x\in [0, 1].\]</div>
<p>This function has both local minimum and global minimum. The global
minimum is <span class="math notranslate">\(0.75725\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">12</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s take a plot for <span class="math notranslate">\(f\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bo_6_0.png" src="_images/bo_6_0.png" />
</div>
</div>
</div>
<div class="section" id="Set-a-Gaussian-Process-prior">
<h2>Set a Gaussian Process prior<a class="headerlink" href="#Set-a-Gaussian-Process-prior" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian process</a> is
a popular choice for a distribution over functions due to its power and
flexibility (characteristics of a non-parametric model). The core of a
Gaussian Process is its covariance function <span class="math notranslate">\(k\)</span> which governs the
similarity between input points. Here we will use a Gaussian Process
regression as a prior over the objective function <span class="math notranslate">\(f\)</span>. Given
inputs <span class="math notranslate">\(X\)</span> and their noisy observations <span class="math notranslate">\(y\)</span>, the model takes
the form</p>
<div class="math notranslate">
\[f\sim\mathrm{MultivariateNormal}(0,k(X,X)),\]</div>
<div class="math notranslate">
\[y\sim f+\epsilon,\]</div>
<p>where <span class="math notranslate">\(\epsilon\)</span> is noise and <span class="math notranslate">\(k(X,X)\)</span> is a covariance
matrix whose entries are outputs <span class="math notranslate">\(k(x,z)\)</span> of <span class="math notranslate">\(k\)</span> over input
pairs <span class="math notranslate">\((x,z)\)</span>.</p>
<p>The
<a class="reference external" href="https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function">Matern52</a>
kernel will be chose (as suggested in reference [1]) because the popular
<a class="reference external" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">RBF</a>
kernel is so smooth that makes it an unrealistic assumption for the
objective function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># init the model with two input points -0.1 and 1.1</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">gpmodel</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gp</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                 <span class="n">noise</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>The following helper function <code class="docutils literal notranslate"><span class="pre">update_posterior</span></code> will take care of
updating our <code class="docutils literal notranslate"><span class="pre">gpmodel</span></code> when a new <span class="math notranslate">\(x\)</span> is found.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">update_posterior</span><span class="p">(</span><span class="n">x_new</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">x_new</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
    <span class="n">gpmodel</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">gpmodel</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-an-acquisition-function">
<h2>Define an acquisition function<a class="headerlink" href="#Define-an-acquisition-function" title="Permalink to this headline">¶</a></h2>
<p>There are many choices for the acquisition function (see references [1]
and [2] for a list of popular ones and discussions on them). Here we
will use a “simple to implement and interpret” one: Lower Confidence
Bound, which is two standard deviations below posterior mean. Formally,</p>
<div class="math notranslate">
\[\alpha(x) = \mu(x) - \kappa \sigma(x),\quad \kappa = 2,\]</div>
<p>where <span class="math notranslate">\(\mu(x)\)</span> and <span class="math notranslate">\(\sigma(x)\)</span> are mean and variance of the
posterior, and the coefficient <span class="math notranslate">\(\kappa\)</span> plays a trade-off between
exploitation-exploration. A high <span class="math notranslate">\(\kappa\)</span> means that we play more
weight on exploration (because we seek minimum candidates in a
higher-uncertainty area). Otherwise, we do exploitation: seek minimum
candidates around the posterior mean <span class="math notranslate">\(\mu(x)\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">lower_confidence_bound</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">gpmodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">noiseless</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">variance</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">sigma</span>
</pre></div>
</div>
</div>
<p>The final component is a way to find a minimum point for acquisition
function. There are several ways for us to choose: gradient-based and
non-gradient-based. Here, we will follow the gradient-based approach.
One of the drawback of gradient descent methods is it is easy for the
minimizer to get stuck at a local minimum. In this tutorial, we will use
a simple way to solve that problem. First, draw <span class="math notranslate">\(3\)</span> samples from
our Gaussian Process posteriors and find a candidate for each sample.
Then (after collecting <span class="math notranslate">\(3\)</span> candidates), we select the minimum
point among them. A more detailed discussion regarding to this problem
in Bayesian Optimization is presented in the reference [2].</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">find_a_candidate</span><span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">lower_bound</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># transform x to an unconstrained domain to set an minimizer for it</span>
    <span class="n">constraint</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>
    <span class="n">unconstrained_x_init</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">x_init</span><span class="p">)</span>
    <span class="n">unconstrained_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">unconstrained_x_init</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">minimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">([</span><span class="n">unconstrained_x</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">minimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">constraint</span><span class="p">)(</span><span class="n">unconstrained_x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">unconstrained_x</span><span class="p">,</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">unconstrained_x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="n">minimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
    <span class="c1"># after a candidate found in unconstrained domain, convert it back to original domain</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">constraint</span><span class="p">)(</span><span class="n">unconstrained_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">next_x</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_candidates</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># last data point will be an init point for first minimum candidate,</span>
    <span class="c1"># other minimum candidates will get uniform random initialization</span>
    <span class="n">x_init</span> <span class="o">=</span> <span class="n">gpmodel</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_candidates</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">find_a_candidate</span><span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">x_init</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>

    <span class="n">argmin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">candidates</span><span class="p">[</span><span class="n">argmin</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Find-">
<h2>Find <span class="math notranslate">\(\{ x_n \}\)</span><a class="headerlink" href="#Find-" title="Permalink to this headline">¶</a></h2>
<p>To illustrate how Bayesian Optimization works, we make a convenient
plotting function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">gs</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">with_title</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;xmin&quot;</span> <span class="k">if</span> <span class="n">xlabel</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="s2">&quot;x{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
    <span class="n">Xnew</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">gpmodel</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">&quot;kx&quot;</span><span class="p">)</span>  <span class="c1"># plot all observed data</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">loc</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">gpmodel</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">noiseless</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">sd</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># plot prediction mean</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">sd</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">sd</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                         <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>  <span class="c1"># plot uncertainty intervals</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Find {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xlabel</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">with_title</span><span class="p">:</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Gaussian Process Regression&quot;</span><span class="p">)</span>

    <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># plot acquisition</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xmin</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">xmin</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s2">&quot;{} = {:.5f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xlabel</span><span class="p">,</span> <span class="n">xmin</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>  <span class="c1"># plot minimum point</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_title</span><span class="p">:</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Lower Confidence Bound&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>First, we generate a random point <code class="docutils literal notranslate"><span class="pre">x0</span></code> in the interval <span class="math notranslate">\([0, 1]\)</span>.
Then use <code class="docutils literal notranslate"><span class="pre">update_posterior</span></code> and <code class="docutils literal notranslate"><span class="pre">next_x</span></code> functions repeatly to do
Bayesian optimization for <span class="math notranslate">\(f\)</span>. The following plot illustrates how
Gaussian Process posteriors and their derived acquisition functions
change when we observe more data for the next <span class="math notranslate">\(8\)</span> steps.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython2 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">30</span><span class="p">))</span>
<span class="n">outer_gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="n">x0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">update_posterior</span><span class="p">(</span><span class="n">xmin</span><span class="p">)</span>
    <span class="n">xmin</span> <span class="o">=</span> <span class="n">next_x</span><span class="p">()</span>
    <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpecFromSubplotSpec</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">subplot_spec</span><span class="o">=</span><span class="n">outer_gs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">gs</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">with_title</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bo_22_0.png" src="_images/bo_22_0.png" />
</div>
</div>
<p>Because we made an assumption that our observations contain noise, it is
improbable to require an exact result. However, we can see that the
sequence has converged to near the global minimum <span class="math notranslate">\(0.75725\)</span>.
That’s it! Through this tutorial, we hope that we have conveyed some
basic ideas of Bayesian optimization. For more reference, the youtube
video [3] is an excellent course to learn the basic theory. And the
paper [2] gives a review of current progresses on this subject, together
with many discussions about technical details.</p>
</div>
<div class="section" id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<p>[1]
<code class="docutils literal notranslate"><span class="pre">Practical</span> <span class="pre">bayesian</span> <span class="pre">optimization</span> <span class="pre">of</span> <span class="pre">machine</span> <span class="pre">learning</span> <span class="pre">algorithms</span></code>,
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams</p>
<p>[2]
<code class="docutils literal notranslate"><span class="pre">Taking</span> <span class="pre">the</span> <span class="pre">human</span> <span class="pre">out</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">loop:</span> <span class="pre">A</span> <span class="pre">review</span> <span class="pre">of</span> <span class="pre">bayesian</span> <span class="pre">optimization</span></code>,
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando De
Freitas</p>
<p>[3] <a class="reference external" href="https://www.youtube.com/watch?v=vz3D36VXefI">Machine learning - Bayesian optimization and multi-armed
bandits</a></p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="gp.html" class="btn btn-neutral" title="Gaussian Processes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2018, Uber Technologies, Inc.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>