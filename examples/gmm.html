

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gaussian Mixture Model &mdash; Pyro Tutorials 0.2.0-a0+88254ac documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Pyro Tutorials 0.2.0-a0+88254ac documentation" href="index.html"/>
        <link rel="next" title="Gaussian Processes" href="gp.html"/>
        <link rel="prev" title="The Semi-Supervised VAE" href="ss-vae.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                pytorch-0.3-279-gdafa1ddd
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Models in Pyro: From Primitive Distributions to Stochastic Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Inference in Pyro: From Stochastic Functions to Marginal Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro 0.2</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">Bayesian Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gaussian Mixture Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Dataset">Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Maximum-likelihood-approach">Maximum likelihood approach</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Gaussian Mixture Model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/gmm.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Gaussian-Mixture-Model">
<h1>Gaussian Mixture Model<a class="headerlink" href="#Gaussian-Mixture-Model" title="Permalink to this headline">¶</a></h1>
<p>This is a brief tutorial on training mixture models in Pyro. We’ll focus
on the mechanics of <code class="docutils literal"><span class="pre">config_enumerate()</span></code> and setting up mixture
weights. To simplify matters, we’ll train a trivial 1-D Gaussian model
on a tiny 5-point dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>from __future__ import print_function
import os
from collections import defaultdict
import numpy as np
import scipy.stats
import torch
from torch.distributions import constraints
from matplotlib import pyplot
%matplotlib inline

import pyro
import pyro.distributions as dist
from pyro.optim import Adam
from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate

smoke_test = (&#39;CI&#39; in os.environ)
pyro.enable_validation(True)
</pre></div>
</div>
</div>
<div class="section" id="Dataset">
<h2>Dataset<a class="headerlink" href="#Dataset" title="Permalink to this headline">¶</a></h2>
<p>Here is our tiny dataset. It has five points.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>data = torch.tensor([0., 1., 10., 11., 12.])
</pre></div>
</div>
</div>
</div>
<div class="section" id="Maximum-likelihood-approach">
<h2>Maximum likelihood approach<a class="headerlink" href="#Maximum-likelihood-approach" title="Permalink to this headline">¶</a></h2>
<p>Let’s start by optimizing model parameters <code class="docutils literal"><span class="pre">weights</span></code>, <code class="docutils literal"><span class="pre">locs</span></code>, and
<code class="docutils literal"><span class="pre">scale</span></code>, rather than treating them as random variables with priors.
Our model will learn global mixture weights, the location of each
mixture component, and a shared scale that is common to both components.
Our guide will learn soft assignment weights of each point.</p>
<p>Note that none of our parameters have priors. In this Maximum Likelihood
approach we can embed our parameters directly in the model rather than
the guide. This is equivalent to adding them in the guide as
<code class="docutils literal"><span class="pre">pyro.sample(...,</span> <span class="pre">dist.Delta(...))</span></code> sites and using a uniform prior in
the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>K = 2  # Fixed number of components.

def model(data):
    # Global parameters.
    weights = pyro.param(&#39;weights&#39;, torch.ones(K) / K, constraint=constraints.simplex)
    locs = pyro.param(&#39;locs&#39;, 10 * torch.randn(K))
    scale = pyro.param(&#39;scale&#39;, torch.tensor(0.5), constraint=constraints.positive)

    with pyro.iarange(&#39;data&#39;):
        # Local variables.
        assignment = pyro.sample(&#39;assignment&#39;,
                                 dist.Categorical(weights).expand_by([len(data)]))
        pyro.sample(&#39;obs&#39;, dist.Normal(locs[assignment], scale), obs=data)

def guide(data):
    with pyro.iarange(&#39;data&#39;):
        # Local parameters.
        assignment_probs = pyro.param(&#39;assignment_probs&#39;, torch.ones(len(data), K) / K,
                                      constraint=constraints.unit_interval)
        pyro.sample(&#39;assignment&#39;, dist.Categorical(assignment_probs))
</pre></div>
</div>
</div>
<p>To run inference with this <code class="docutils literal"><span class="pre">(model,guide)</span></code> pair, we use Pyro’s
<code class="docutils literal"><span class="pre">config_enumerate()</span></code> function to enumerate over all assignments in
each iteration. Since we’ve wrapped the batched Categorical assignments
in a <code class="docutils literal"><span class="pre">pyro.iarange</span></code> indepencence context, this enumeration can happen
in parallel: we enumerate only 2 possibilites, rather than
<code class="docutils literal"><span class="pre">2**len(data)</span> <span class="pre">=</span> <span class="pre">32</span></code>. Finally, to use the parallel version of
enumeration, we inform pyro that we’re only using a single <code class="docutils literal"><span class="pre">iarange</span></code>
via <code class="docutils literal"><span class="pre">max_iarange_nesting=1</span></code>; this lets Pyro know that we’re using the
rightmost dimension <code class="docutils literal"><span class="pre">iarange</span></code> and letting use any other dimension for
parallelization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>optim = pyro.optim.Adam({&#39;lr&#39;: 0.2, &#39;betas&#39;: [0.9, 0.99]})
inference = SVI(model, config_enumerate(guide, &#39;parallel&#39;), optim,
                loss=TraceEnum_ELBO(max_iarange_nesting=1))
</pre></div>
</div>
</div>
<p>During training, we’ll collect both losses and gradient norms to monitor
convergence. We can do this using PyTorch’s <code class="docutils literal"><span class="pre">.register_hook()</span></code> method.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>pyro.set_rng_seed(1)      # Set seed to make results reproducible.
pyro.clear_param_store()  # Clear stale param values.

# Register hooks to monitor gradient norms.
gradient_norms = defaultdict(list)
inference.loss(model, guide, data)  # Initializes param store.
for name, value in pyro.get_param_store().named_parameters():
    value.register_hook(lambda g, name=name: gradient_norms[name].append(g.norm().item()))

losses = []
for i in range(500 if not smoke_test else 2):
    loss = inference.step(data)
    losses.append(loss)
    print(&#39;.&#39; if i % 100 else &#39;\n&#39;, end=&#39;&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

...................................................................................................
...................................................................................................
...................................................................................................
...................................................................................................
...................................................................................................
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>pyplot.figure(figsize=(10,3), dpi=100).set_facecolor(&#39;white&#39;)
pyplot.plot(losses)
pyplot.xlabel(&#39;iters&#39;)
pyplot.ylabel(&#39;loss&#39;)
pyplot.yscale(&#39;log&#39;)
pyplot.title(&#39;Convergence of SVI&#39;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gmm_10_0.png" src="_images/gmm_10_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>pyplot.figure(figsize=(10,4), dpi=100).set_facecolor(&#39;white&#39;)
for name, grad_norms in gradient_norms.items():
    pyplot.plot(grad_norms, label=name)
pyplot.xlabel(&#39;iters&#39;)
pyplot.ylabel(&#39;gradient norm&#39;)
pyplot.yscale(&#39;log&#39;)
pyplot.legend(loc=&#39;best&#39;)
pyplot.title(&#39;Gradient norms during SVI&#39;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gmm_11_0.png" src="_images/gmm_11_0.png" />
</div>
</div>
<p>Here are the learned parameters:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>weights = pyro.param(&#39;weights&#39;)
locs = pyro.param(&#39;locs&#39;)
scale = pyro.param(&#39;scale&#39;)
print(&#39;weights = {}&#39;.format(weights.data.numpy()))
print(&#39;locs = {}&#39;.format(locs.data.numpy()))
print(&#39;scale = {}&#39;.format(scale.data.numpy()))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
weights = [0.59612644 0.4038736 ]
locs = [10.999848    0.50015897]
scale = 0.708275258541
</pre></div></div>
</div>
<p>The model’s <code class="docutils literal"><span class="pre">weights</span></code> are as expected, with 3/5 of the data in the
first component and 2/3 in the second component. We can also examine the
guide’s local <code class="docutils literal"><span class="pre">assignment_probs</span></code> variable.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>assignment_probs = pyro.param(&#39;assignment_probs&#39;)
pyplot.figure(figsize=(8, 4), dpi=100).set_facecolor(&#39;white&#39;)
pyplot.plot(data.data.numpy(), assignment_probs.data.numpy()[:, 0], &#39;ro&#39;,
            label=&#39;component with mean {:0.2g}&#39;.format(locs[0]))
pyplot.plot(data.data.numpy(), assignment_probs.data.numpy()[:, 1], &#39;bo&#39;,
            label=&#39;component with mean {:0.2g}&#39;.format(locs[1]))
pyplot.title(&#39;Mixture assignment probabilities&#39;)
pyplot.xlabel(&#39;data value&#39;)
pyplot.ylabel(&#39;assignment probability&#39;)
pyplot.legend(loc=&#39;center&#39;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gmm_15_0.png" src="_images/gmm_15_0.png" />
</div>
</div>
<p>Next let’s visualize the mixture model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>X = np.arange(-3,15,0.1)
Y1 = weights[0].item() * scipy.stats.norm.pdf((X - locs[0].item()) / scale.item())
Y2 = weights[1].item() * scipy.stats.norm.pdf((X - locs[1].item()) / scale.item())

pyplot.figure(figsize=(10, 4), dpi=100).set_facecolor(&#39;white&#39;)
pyplot.plot(X, Y1, &#39;r-&#39;)
pyplot.plot(X, Y2, &#39;b-&#39;)
pyplot.plot(X, Y1 + Y2, &#39;k--&#39;)
pyplot.plot(data.data.numpy(), np.zeros(len(data)), &#39;k*&#39;)
pyplot.title(&#39;Densitiy of two-component mixture model&#39;)
pyplot.ylabel(&#39;probability density&#39;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gmm_17_0.png" src="_images/gmm_17_0.png" />
</div>
</div>
<p>Finally note that optimization with mixture models is non-convex and can
often get stuck in local optima. For example in this tutorial, we
observed that the mixture model gets stuck in an
everthing-in-one-cluster hypothesis if <code class="docutils literal"><span class="pre">scale</span></code> is initialized to be
too large.</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gp.html" class="btn btn-neutral float-right" title="Gaussian Processes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ss-vae.html" class="btn btn-neutral" title="The Semi-Supervised VAE" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2018, Uber Technologies, Inc.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2.0-a0+88254ac',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>