

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Bayesian Regression &mdash; Pyro Tutorials 0.2.0-a0+88254ac documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Pyro Tutorials 0.2.0-a0+88254ac documentation" href="index.html"/>
        <link rel="next" title="Deep Markov Model" href="dmm.html"/>
        <link rel="prev" title="Variational Autoencoders" href="vae.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                pytorch-0.3-279-gdafa1ddd
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Models in Pyro: From Primitive Distributions to Stochastic Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Inference in Pyro: From Stochastic Functions to Marginal Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro 0.2</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Regression">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Bayesian-Regression">Bayesian Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#random_module()"><code class="docutils literal"><span class="pre">random_module()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#Model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Guide">Guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Validating-Results">Validating Results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Bayesian Regression</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/bayesian_regression.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Bayesian-Regression">
<h1>Bayesian Regression<a class="headerlink" href="#Bayesian-Regression" title="Permalink to this headline">¶</a></h1>
<p>Regression is one of the most common and basic supervised learning tasks
in machine learning. Suppose we’re given a dataset <span class="math">\(\mathcal{D}\)</span>
of the form</p>
<div class="math">
\[\mathcal{D}  = \{ (X_i, y_i) \} \qquad \text{for}\qquad i=1,2,...,N\]</div>
<p>The goal of linear regression is to fit a function to the data of the
form:</p>
<div class="math">
\[y = w X + b + \epsilon\]</div>
<p>where <span class="math">\(w\)</span> and <span class="math">\(b\)</span> are learnable parameters and
<span class="math">\(\epsilon\)</span> represents observation noise. Specifically <span class="math">\(w\)</span> is
a matrix of weights and <span class="math">\(b\)</span> is a bias vector.</p>
<p>Let’s first implement linear regression in PyTorch and learn point
estimates for the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span>. Then we’ll see how
to incorporate uncertainty into our estimates by using Pyro to implement
Bayesian linear regression.</p>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">¶</a></h2>
<p>As always, let’s begin by importing the modules we’ll need.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>import os
import numpy as np
import torch
import torch.nn as nn

import pyro
from pyro.distributions import Normal
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam
# for CI testing
smoke_test = (&#39;CI&#39; in os.environ)
pyro.enable_validation(True)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Data">
<h2>Data<a class="headerlink" href="#Data" title="Permalink to this headline">¶</a></h2>
<p>We’ll generate a toy dataset with one feature and <span class="math">\(w = 3\)</span> and
<span class="math">\(b = 1\)</span> as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>N = 100  # size of toy data

def build_linear_dataset(N, p=1, noise_std=0.01):
    X = np.random.rand(N, p)
    # w = 3
    w = 3 * np.ones(p)
    # b = 1
    y = np.matmul(X, w) + np.repeat(1, N) + np.random.normal(0, noise_std, size=N)
    y = y.reshape(N, 1)
    X, y = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)
    data = torch.cat((X, y), 1)
    assert data.shape == (N, p + 1)
    return data
</pre></div>
</div>
</div>
<p>Note that we generate the data with a fixed observation noise
<span class="math">\(\sigma = 0.1\)</span>.</p>
</div>
<div class="section" id="Regression">
<h2>Regression<a class="headerlink" href="#Regression" title="Permalink to this headline">¶</a></h2>
<p>Now let’s define our regression model. We’ll use PyTorch’s <code class="docutils literal"><span class="pre">nn.Module</span></code>
for this. Our input <span class="math">\(X\)</span> is a matrix of size <span class="math">\(N \times p\)</span> and
our output <span class="math">\(y\)</span> is a vector of size <span class="math">\(p \times 1\)</span>. The
function <code class="docutils literal"><span class="pre">nn.Linear(p,</span> <span class="pre">1)</span></code> defines a linear transformation of the form
<span class="math">\(Xw + b\)</span> where <span class="math">\(w\)</span> is the weight matrix and <span class="math">\(b\)</span> is the
additive bias. As you can see, we can easily make this a logistic
regression by adding a non-linearity in the <code class="docutils literal"><span class="pre">forward()</span></code> method.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>class RegressionModel(nn.Module):
    def __init__(self, p):
        # p = number of features
        super(RegressionModel, self).__init__()
        self.linear = nn.Linear(p, 1)

    def forward(self, x):
        return self.linear(x)

regression_model = RegressionModel(1)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training">
<h2>Training<a class="headerlink" href="#Training" title="Permalink to this headline">¶</a></h2>
<p>We will use the mean squared error (MSE) as our loss and Adam as our
optimizer. We would like to optimize the parameters of the
<code class="docutils literal"><span class="pre">regression_model</span></code> neural net above. We will use a somewhat large
learning rate of <code class="docutils literal"><span class="pre">0.01</span></code> and run for 500 iterations.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>loss_fn = torch.nn.MSELoss(size_average=False)
optim = torch.optim.Adam(regression_model.parameters(), lr=0.05)
num_iterations = 1000 if not smoke_test else 2

def main():
    data = build_linear_dataset(N)
    x_data = data[:, :-1]
    y_data = data[:, -1]
    for j in range(num_iterations):
        # run the model forward on the data
        y_pred = regression_model(x_data).squeeze(-1)
        # calculate the mse loss
        loss = loss_fn(y_pred, y_data)
        # initialize gradients to zero
        optim.zero_grad()
        # backpropagate
        loss.backward()
        # take a gradient step
        optim.step()
        if (j + 1) % 50 == 0:
            print(&quot;[iteration %04d] loss: %.4f&quot; % (j + 1, loss.item()))
    # Inspect learned parameters
    print(&quot;Learned parameters:&quot;)
    for name, param in regression_model.named_parameters():
        print(&quot;%s: %.3f&quot; % (name, param.data.numpy()))

if __name__ == &#39;__main__&#39;:
    main()
</pre></div>
</div>
</div>
<p><strong>Sample Output</strong>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">iteration</span> <span class="mi">0400</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0105</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0450</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0096</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0500</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0550</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0600</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0650</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0700</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0750</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0800</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0850</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0900</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">0950</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="n">iteration</span> <span class="mi">1000</span><span class="p">]</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0095</span>
<span class="n">Learned</span> <span class="n">parameters</span><span class="p">:</span>
<span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="mf">3.004</span>
<span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span> <span class="mf">0.997</span>
</pre></div>
</div>
<p>Not too bad - you can see that the regressor learned parameters that
were pretty close to the ground truth of <span class="math">\(w = 3, b = 1\)</span>. But how
confident should we be in these point estimates?</p>
<p><a class="reference external" href="http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf">Bayesian
modeling</a>
offers a systematic framework for reasoning about model uncertainty.
Instead of just learning point estimates, we’re going to learn a
<em>distribution</em> over values of the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span>
that are consistent with the observed data.</p>
</div>
<div class="section" id="Bayesian-Regression">
<h2>Bayesian Regression<a class="headerlink" href="#Bayesian-Regression" title="Permalink to this headline">¶</a></h2>
<p>In order to make our linear regression Bayesian, we need to put priors
on the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span>. These are distributions that
represent our prior belief about reasonable values for <span class="math">\(w\)</span> and
<span class="math">\(b\)</span> (before observing any data).</p>
<div class="section" id="random_module()">
<h3><code class="docutils literal"><span class="pre">random_module()</span></code><a class="headerlink" href="#random_module()" title="Permalink to this headline">¶</a></h3>
<p>In order to do this, we’ll ‘lift’ the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span>
to random variables. We can do this in Pyro via <code class="docutils literal"><span class="pre">random_module()</span></code>,
which effectively takes a given <code class="docutils literal"><span class="pre">nn.Module</span></code> and turns it into a
distribution over the same module; in our case, this will be a
distribution over regressors. Specifically, each parameter in the
original regression model is sampled from the provided prior. This
allows us to repurpose vanilla regression models for use in the Bayesian
setting. For example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>loc = torch.zeros(1, 1)
scale = torch.ones(1, 1)
# define a unit normal prior
prior = Normal(loc, scale)
# overload the parameters in the regression module with samples from the prior
lifted_module = pyro.random_module(&quot;regression_module&quot;, regression_model, prior)
# sample a regressor from the prior
sampled_reg_model = lifted_module()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Model">
<h3>Model<a class="headerlink" href="#Model" title="Permalink to this headline">¶</a></h3>
<p>We now have all the ingredients needed to specify our model. First we
define priors over <span class="math">\(w\)</span> and <span class="math">\(b\)</span>. Because we’re uncertain
about the parameters a priori, we’ll use relatively wide priors
<span class="math">\(\mathcal{N}(\mu = 0, \sigma = 10)\)</span>. Then we wrap
<code class="docutils literal"><span class="pre">regression_model</span></code> with <code class="docutils literal"><span class="pre">random_module</span></code> and sample an instance of
the regressor, <code class="docutils literal"><span class="pre">lifted_reg_model</span></code>. We then run the regressor forward
on the inputs <code class="docutils literal"><span class="pre">x_data</span></code>. Finally we use the <code class="docutils literal"><span class="pre">obs</span></code> argument to the
<code class="docutils literal"><span class="pre">pyro.sample</span></code> statement to condition on the observed data <code class="docutils literal"><span class="pre">y_data</span></code>.
Note that we use the same fixed observation noise that was used to
generate the data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>def model(data):
    # Create unit normal priors over the parameters
    loc, scale = torch.zeros(1, 1), 10 * torch.ones(1, 1)
    bias_loc, bias_scale = torch.zeros(1), 10 * torch.ones(1)
    w_prior = Normal(loc, scale).independent(1)
    b_prior = Normal(bias_loc, bias_scale).independent(1)
    priors = {&#39;linear.weight&#39;: w_prior, &#39;linear.bias&#39;: b_prior}
    # lift module parameters to random variables sampled from the priors
    lifted_module = pyro.random_module(&quot;module&quot;, regression_model, priors)
    # sample a regressor (which also samples w and b)
    lifted_reg_model = lifted_module()
    with pyro.iarange(&quot;map&quot;, N):
        x_data = data[:, :-1]
        y_data = data[:, -1]

        # run the regressor forward conditioned on data
        prediction_mean = lifted_reg_model(x_data).squeeze(-1)
        # condition on the observed data
        pyro.sample(&quot;obs&quot;,
                    Normal(prediction_mean, 0.1 * torch.ones(data.size(0))),
                    obs=y_data)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Guide">
<h3>Guide<a class="headerlink" href="#Guide" title="Permalink to this headline">¶</a></h3>
<p>In order to do inference we’re going to need a guide, i.e.&nbsp;a
parameterized family of distributions over <span class="math">\(w\)</span> and <span class="math">\(b\)</span>.
Writing down a guide will proceed in close analogy to the construction
of our model, with the key difference that the guide parameters need to
be trainable. To do this we register the guide parameters in the
ParamStore using <code class="docutils literal"><span class="pre">pyro.param()</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>softplus = torch.nn.Softplus()

def guide(data):
    # define our variational parameters
    w_loc = torch.randn(1, 1)
    # note that we initialize our scales to be pretty narrow
    w_log_sig = torch.tensor(-3.0 * torch.ones(1, 1) + 0.05 * torch.randn(1, 1))
    b_loc = torch.randn(1)
    b_log_sig = torch.tensor(-3.0 * torch.ones(1) + 0.05 * torch.randn(1))
    # register learnable params in the param store
    mw_param = pyro.param(&quot;guide_mean_weight&quot;, w_loc)
    sw_param = softplus(pyro.param(&quot;guide_log_scale_weight&quot;, w_log_sig))
    mb_param = pyro.param(&quot;guide_mean_bias&quot;, b_loc)
    sb_param = softplus(pyro.param(&quot;guide_log_scale_bias&quot;, b_log_sig))
    # guide distributions for w and b
    w_dist = Normal(mw_param, sw_param).independent(1)
    b_dist = Normal(mb_param, sb_param).independent(1)
    dists = {&#39;linear.weight&#39;: w_dist, &#39;linear.bias&#39;: b_dist}
    # overload the parameters in the module with random samples
    # from the guide distributions
    lifted_module = pyro.random_module(&quot;module&quot;, regression_model, dists)
    # sample a regressor (which also samples w and b)
    return lifted_module()
</pre></div>
</div>
</div>
<p>Note that we choose Gaussians for both guide distributions. Also, to
ensure positivity, we pass each log scale through a <code class="docutils literal"><span class="pre">softplus()</span></code>
transformation (an alternative to ensure positivity would be an
<code class="docutils literal"><span class="pre">exp()</span></code>-transformation).</p>
</div>
</div>
<div class="section" id="Inference">
<h2>Inference<a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h2>
<p>To do inference we’ll use stochastic variational inference (SVI) (for an
introduction to SVI, see <a class="reference internal" href="svi_part_i.html"><span class="doc">SVI Part I</span></a>). Just like
in the non-Bayesian linear regression, each iteration of our training
loop will take a gradient step, with the difference that in this case,
we’ll use the ELBO objective instead of the MSE loss by constructing a
<code class="docutils literal"><span class="pre">Trace_ELBO</span></code> object that we pass to <code class="docutils literal"><span class="pre">SVI</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>optim = Adam({&quot;lr&quot;: 0.05})
svi = SVI(model, guide, optim, loss=Trace_ELBO())
</pre></div>
</div>
</div>
<p>Here <code class="docutils literal"><span class="pre">Adam</span></code> is a thin wrapper around <code class="docutils literal"><span class="pre">torch.optim.Adam</span></code> (see
<a class="reference internal" href="svi_part_i.html#Optimizers"><span class="std std-ref">here</span></a> for a discussion). The complete
training loop is as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>def main():
    pyro.clear_param_store()
    data = build_linear_dataset(N)
    for j in range(num_iterations):
        # calculate the loss and take a gradient step
        loss = svi.step(data)
        if j % 100 == 0:
            print(&quot;[iteration %04d] loss: %.4f&quot; % (j + 1, loss / float(N)))

if __name__ == &#39;__main__&#39;:
    main()
</pre></div>
</div>
</div>
<p>To take an ELBO gradient step we simply call the <code class="docutils literal"><span class="pre">step</span></code> method of
<code class="docutils literal"><span class="pre">SVI</span></code>. Notice that the <code class="docutils literal"><span class="pre">data</span></code> argument we pass to <code class="docutils literal"><span class="pre">step</span></code> will be
passed to both <code class="docutils literal"><span class="pre">model()</span></code> and <code class="docutils literal"><span class="pre">guide()</span></code>.</p>
</div>
<div class="section" id="Validating-Results">
<h2>Validating Results<a class="headerlink" href="#Validating-Results" title="Permalink to this headline">¶</a></h2>
<p>Let’s compare the variational parameters we learned to our previous
result:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>for name in pyro.get_param_store().get_all_param_names():
    print(&quot;[%s]: %.3f&quot; % (name, pyro.param(name).data.numpy()))
</pre></div>
</div>
</div>
<p><strong>Sample Output</strong>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">guide_log_scale_weight</span><span class="p">]:</span> <span class="o">-</span><span class="mf">3.217</span>
<span class="p">[</span><span class="n">guide_log_scale_bias</span><span class="p">]:</span> <span class="o">-</span><span class="mf">3.164</span>
<span class="p">[</span><span class="n">guide_mean_weight</span><span class="p">]:</span> <span class="mf">2.966</span>
<span class="p">[</span><span class="n">guide_mean_bias</span><span class="p">]:</span> <span class="mf">0.941</span>
</pre></div>
</div>
<p>As you can see, the means of our parameter estimates are pretty close to
the values we previously learned. Now, however, instead of just point
estimates, the parameters <code class="docutils literal"><span class="pre">guide_log_scale_weight</span></code> and
<code class="docutils literal"><span class="pre">guide_log_scale_bias</span></code> provide us with uncertainty estimates. (Note
that the scales are in log-space here, so the more negative the value,
the narrower the width).</p>
<p>Finally, let’s evaluate our model by checking its predictive accuracy on
new test data. This is known as <em>point evaluation</em>. We’ll sample 20
neural nets from our posterior, run them on the test data, then average
across their predictions and calculate the MSE of the predicted values
compared to the ground truth.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>X = np.linspace(6, 7, num=20)
y = 3 * X + 1
X, y = X.reshape((20, 1)), y.reshape((20, 1))
x_data, y_data = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)
loss = nn.MSELoss()
y_preds = torch.zeros(20, 1)
for i in range(20):
    # guide does not require the data
    sampled_reg_model = guide(None)
    # run the regression model and add prediction to total
    y_preds = y_preds + sampled_reg_model(x_data)
# take the average of the predictions
y_preds = y_preds / 20
print (&quot;Loss: &quot;, loss(y_preds, y_data).item())
</pre></div>
</div>
</div>
<p><strong>Sample Output</strong>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Loss</span><span class="p">:</span>  <span class="mf">0.00025596367777325213</span>
</pre></div>
</div>
<p>See the full code on
<a class="reference external" href="https://github.com/uber/pyro/blob/dev/examples/bayesian_regression.py">Github</a>.</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dmm.html" class="btn btn-neutral float-right" title="Deep Markov Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="vae.html" class="btn btn-neutral" title="Variational Autoencoders" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2018, Uber Technologies, Inc.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2.0-a0+88254ac',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>