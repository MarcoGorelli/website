

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Bayesian Regression - Introduction (Part 1) &mdash; Pyro Tutorials 0.4.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Pyro Tutorials 0.4.0 documentation" href="index.html"/>
        <link rel="next" title="Bayesian Regression - Inference Algorithms (Part 2)" href="bayesian_regression_ii.html"/>
        <link rel="prev" title="Variational Autoencoders" href="vae.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.4.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">An Introduction to Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">An Introduction to Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enumeration.html">Inference with Discrete Latent Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_objectives.html">Custom SVI Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">Using the PyTorch JIT Compiler with Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="effect_handlers.html">Poutine: A Guide to Programming with Effect Handlers in Pyro</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Regression - Introduction (Part 1)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Setup">Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Dataset">Dataset</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Linear-Regression">Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Bayesian-Regression">Bayesian Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#random_module()"><code class="docutils literal notranslate"><span class="pre">random_module()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#Model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Guide">Guide</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Model-Evaluation">Model Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression_ii.html">Bayesian Regression - Inference Algorithms (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gplvm.html">Gaussian Process Latent Variable Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="easyguide.html">Writing guides using EasyGuide</a></li>
<li class="toctree-l1"><a class="reference internal" href="tracking_1d.html">Tracking an Unknown Number of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="csis.html">Compiled Sequential Importance Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-implicature.html">The Rational Speech Act framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-hyperbole.html">Understanding Hyperbole using RSA</a></li>
<li class="toctree-l1"><a class="reference internal" href="ekf.html">Kalman Filter</a></li>
</ul>
<p class="caption"><span class="caption-text">Code Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Hidden Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Latent Dirichlet Allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Sparse Gamma Deep Exponential Family</a></li>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Deep Kernel Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Plated Einsum</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Bayesian Regression - Introduction (Part 1)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/bayesian_regression.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Bayesian-Regression---Introduction-(Part-1)">
<h1>Bayesian Regression - Introduction (Part 1)<a class="headerlink" href="#Bayesian-Regression---Introduction-(Part-1)" title="Permalink to this headline">¶</a></h1>
<p>Regression is one of the most common and basic supervised learning tasks
in machine learning. Suppose we’re given a dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
of the form</p>
<div class="math notranslate nohighlight">
\[\mathcal{D}  = \{ (X_i, y_i) \} \qquad \text{for}\qquad i=1,2,...,N\]</div>
<p>The goal of linear regression is to fit a function to the data of the
form:</p>
<div class="math notranslate nohighlight">
\[y = w X + b + \epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are learnable parameters and
<span class="math notranslate nohighlight">\(\epsilon\)</span> represents observation noise. Specifically <span class="math notranslate nohighlight">\(w\)</span> is
a matrix of weights and <span class="math notranslate nohighlight">\(b\)</span> is a bias vector.</p>
<p>Let’s first implement linear regression in PyTorch and learn point
estimates for the parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. Then we’ll see how
to incorporate uncertainty into our estimates by using Pyro to implement
Bayesian regression.</p>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">¶</a></h2>
<p>Let’s begin by importing the modules we’ll need.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="k">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">from</span> <span class="nn">pyro.distributions</span> <span class="k">import</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">Uniform</span><span class="p">,</span> <span class="n">Delta</span>
<span class="kn">from</span> <span class="nn">pyro.infer</span> <span class="k">import</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span>
<span class="kn">from</span> <span class="nn">pyro.optim</span> <span class="k">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">pyro.distributions.util</span> <span class="k">import</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">pyro.infer</span> <span class="k">import</span> <span class="n">EmpiricalMarginal</span><span class="p">,</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span><span class="p">,</span> <span class="n">TracePredictive</span>
<span class="kn">from</span> <span class="nn">pyro.infer.mcmc</span> <span class="k">import</span> <span class="n">MCMC</span><span class="p">,</span> <span class="n">NUTS</span>
<span class="kn">import</span> <span class="nn">pyro.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">pyro.poutine</span> <span class="k">as</span> <span class="nn">poutine</span>

<span class="c1"># for CI testing</span>
<span class="n">smoke_test</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;CI&#39;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">pyro</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;0.4.0&#39;</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">enable_validation</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">set_rng_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">enable_validation</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Dataset">
<h3>Dataset<a class="headerlink" href="#Dataset" title="Permalink to this headline">¶</a></h3>
<p>The following example is adapted from [1]. We would like to explore the
relationship between topographic heterogeneity of a nation as measured
by the Terrain Ruggedness Index (variable <em>rugged</em> in the dataset) and
its GDP per capita. In particular, it was noted by the authors in [1]
that terrain ruggedness or bad geography is related to poorer economic
performance outside of Africa, but rugged terrains have had a reverse
effect on income for African nations. Let us look at the data [2] and
investigate this relationship. We will be focusing on three features
from the dataset: - <code class="docutils literal notranslate"><span class="pre">rugged</span></code>: quantifies the Terrain Ruggedness Index
- <code class="docutils literal notranslate"><span class="pre">cont_africa</span></code>: whether the given nation is in Africa -
<code class="docutils literal notranslate"><span class="pre">rgdppc_2000</span></code>: Real GDP per capita for the year 2000</p>
<p>We will take the logarithm for the response variable GDP as it tends to
vary exponentially.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">DATA_URL</span> <span class="o">=</span> <span class="s2">&quot;https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_URL</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;ISO-8859-1&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s2">&quot;cont_africa&quot;</span><span class="p">,</span> <span class="s2">&quot;rugged&quot;</span><span class="p">,</span> <span class="s2">&quot;rgdppc_2000&quot;</span><span class="p">]]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">rgdppc_2000</span><span class="p">)]</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;rgdppc_2000&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;rgdppc_2000&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">african_nations</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;cont_africa&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">non_african_nations</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;cont_africa&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rgdppc_2000&quot;</span><span class="p">]),</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Terrain Ruggedness Index&quot;</span><span class="p">,</span>
          <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;log GDP (2000)&quot;</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Non African Nations&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rgdppc_2000&quot;</span><span class="p">]),</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Terrain Ruggedness Index&quot;</span><span class="p">,</span>
          <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;log GDP (2000)&quot;</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s2">&quot;African Nations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[3]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[Text(0, 0.5, &#39;log GDP (2000)&#39;),
 Text(0.5, 0, &#39;Terrain Ruggedness Index&#39;),
 Text(0.5, 1.0, &#39;African Nations&#39;)]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bayesian_regression_5_1.png" src="_images/bayesian_regression_5_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Linear-Regression">
<h2>Linear Regression<a class="headerlink" href="#Linear-Regression" title="Permalink to this headline">¶</a></h2>
<p>We would like to predict log GDP per capita of a nation as a function of
two features from the dataset - whether the nation is in Africa, and its
Terrain Ruggedness Index. Let’s define our regression model. We’ll use
PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> for this. Our input <span class="math notranslate nohighlight">\(X\)</span> is a matrix of
size <span class="math notranslate nohighlight">\(N \times 2\)</span> and our output <span class="math notranslate nohighlight">\(y\)</span> is a vector of size
<span class="math notranslate nohighlight">\(2 \times 1\)</span>. The function <code class="docutils literal notranslate"><span class="pre">nn.Linear(p,</span> <span class="pre">1)</span></code> defines a linear
transformation of the form <span class="math notranslate nohighlight">\(Xw + b\)</span> where <span class="math notranslate nohighlight">\(w\)</span> is the weight
matrix and <span class="math notranslate nohighlight">\(b\)</span> is the additive bias. We include an extra
<code class="docutils literal notranslate"><span class="pre">self.factor</span></code> term meant to capture the correlation between ruggedness
and whether a country is in Africa.</p>
<p>Note that we can easily make this a logistic regression by adding a
non-linearity in the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">RegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="c1"># p = number of features</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">*</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># number of features</span>
<span class="n">regression_model</span> <span class="o">=</span> <span class="n">RegressionModel</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training">
<h2>Training<a class="headerlink" href="#Training" title="Permalink to this headline">¶</a></h2>
<p>We will use the mean squared error (MSE) as our loss and Adam as our
optimizer. We would like to optimize the parameters of the
<code class="docutils literal notranslate"><span class="pre">regression_model</span></code> neural net above. We will use a somewhat large
learning rate of <code class="docutils literal notranslate"><span class="pre">0.05</span></code> and run for 500 iterations.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">regression_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">1000</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">smoke_test</span> <span class="k">else</span> <span class="mi">2</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="c1"># run the model forward on the data</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">regression_model</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># calculate the mse loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>
        <span class="c1"># initialize gradients to zero</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># backpropagate</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># take a gradient step</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[iteration </span><span class="si">%04d</span><span class="s2">] loss: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="c1"># Inspect learned parameters</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Learned parameters:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[iteration 0050] loss: 3723.7849
[iteration 0100] loss: 1777.9851
[iteration 0150] loss: 1226.7543
[iteration 0200] loss: 927.1514
[iteration 0250] loss: 706.6473
[iteration 0300] loss: 536.0911
[iteration 0350] loss: 408.0940
[iteration 0400] loss: 316.1810
[iteration 0450] loss: 252.9711
[iteration 0500] loss: 211.2545
[iteration 0550] loss: 184.7950
[iteration 0600] loss: 168.6502
[iteration 0650] loss: 159.1673
[iteration 0700] loss: 153.8036
[iteration 0750] loss: 150.8815
[iteration 0800] loss: 149.3482
[iteration 0850] loss: 148.5732
[iteration 0900] loss: 148.1960
[iteration 0950] loss: 148.0193
[iteration 1000] loss: 147.9397
Learned parameters:
factor 0.37248382
linear.weight [[-1.90511    -0.18619268]]
linear.bias [9.188872]
</pre></div></div>
</div>
<p><a class="reference external" href="http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf">Bayesian
modeling</a>
offers a systematic framework for reasoning about model uncertainty.
Instead of just learning point estimates, we’re going to learn a
<em>distribution</em> over variables that are consistent with the observed
data.</p>
</div>
<div class="section" id="Bayesian-Regression">
<h2>Bayesian Regression<a class="headerlink" href="#Bayesian-Regression" title="Permalink to this headline">¶</a></h2>
<p>In order to make our linear regression Bayesian, we need to put priors
on the parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. These are distributions that
represent our prior belief about reasonable values for <span class="math notranslate nohighlight">\(w\)</span> and
<span class="math notranslate nohighlight">\(b\)</span> (before observing any data).</p>
<div class="section" id="random_module()">
<h3><code class="docutils literal notranslate"><span class="pre">random_module()</span></code><a class="headerlink" href="#random_module()" title="Permalink to this headline">¶</a></h3>
<p>In order to do this, we’ll ‘lift’ the parameters of our existing model
to random variables. We can do this in Pyro via <code class="docutils literal notranslate"><span class="pre">random_module()</span></code>,
which effectively takes a given <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and turns it into a
distribution over the same module; in our case, this will be a
distribution over regressors. Specifically, each parameter in the
original regression model is sampled from the provided prior. This
allows us to repurpose vanilla regression models for use in the Bayesian
setting. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># define a unit normal prior</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="c1"># overload the parameters in the regression module with samples from the prior</span>
<span class="n">lifted_module</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">random_module</span><span class="p">(</span><span class="s2">&quot;regression_module&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>
<span class="c1"># sample a nn from the prior</span>
<span class="n">sampled_reg_model</span> <span class="o">=</span> <span class="n">lifted_module</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="Model">
<h3>Model<a class="headerlink" href="#Model" title="Permalink to this headline">¶</a></h3>
<p>We now have all the ingredients needed to specify our model. First we
define priors over weights, biases, and <code class="docutils literal notranslate"><span class="pre">factor</span></code>. Note the priors that
we are using for the different latent variables in the model. The prior
on the intercept parameter is very flat as we would like this to be
learnt from the data. We are using a weakly regularizing prior on the
regression coefficients to avoid overfitting to the data.</p>
<p>We wrap <code class="docutils literal notranslate"><span class="pre">regression_model</span></code> with <code class="docutils literal notranslate"><span class="pre">random_module</span></code> and sample an
instance of the regressor, <code class="docutils literal notranslate"><span class="pre">lifted_reg_model</span></code>. We then run the
regressor on <code class="docutils literal notranslate"><span class="pre">x_data</span></code>. Finally we use the <code class="docutils literal notranslate"><span class="pre">obs</span></code> argument to the
<code class="docutils literal notranslate"><span class="pre">pyro.sample</span></code> statement to condition on the observed data <code class="docutils literal notranslate"><span class="pre">y_data</span></code>
with a learned observation noise <code class="docutils literal notranslate"><span class="pre">sigma</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">):</span>
    <span class="c1"># weight and bias priors</span>
    <span class="n">w_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">b_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">8.</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1000.</span><span class="p">]]))</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">f_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">priors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;linear.weight&#39;</span><span class="p">:</span> <span class="n">w_prior</span><span class="p">,</span> <span class="s1">&#39;linear.bias&#39;</span><span class="p">:</span> <span class="n">b_prior</span><span class="p">,</span> <span class="s1">&#39;factor&#39;</span><span class="p">:</span> <span class="n">f_prior</span><span class="p">}</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;sigma&quot;</span><span class="p">,</span> <span class="n">Uniform</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">))</span>
    <span class="c1"># lift module parameters to random variables sampled from the priors</span>
    <span class="n">lifted_module</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">random_module</span><span class="p">(</span><span class="s2">&quot;module&quot;</span><span class="p">,</span> <span class="n">regression_model</span><span class="p">,</span> <span class="n">priors</span><span class="p">)</span>
    <span class="c1"># sample a nn (which also samples w and b)</span>
    <span class="n">lifted_reg_model</span> <span class="o">=</span> <span class="n">lifted_module</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;map&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_data</span><span class="p">)):</span>
        <span class="c1"># run the nn forward on data</span>
        <span class="n">prediction_mean</span> <span class="o">=</span> <span class="n">lifted_reg_model</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># condition on the observed data</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span>
                    <span class="n">Normal</span><span class="p">(</span><span class="n">prediction_mean</span><span class="p">,</span> <span class="n">scale</span><span class="p">),</span>
                    <span class="n">obs</span><span class="o">=</span><span class="n">y_data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction_mean</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Guide">
<h3>Guide<a class="headerlink" href="#Guide" title="Permalink to this headline">¶</a></h3>
<p>In order to do inference we’re going to need a guide, i.e.&nbsp;a variational
family of distributions. We will use Pyro’s <a class="reference external" href="http://docs.pyro.ai/en/dev/contrib.autoguide.html">autoguide
library</a> to
automatically place Gaussians with diagonal covariance on all of the
distributions in the model. Under the hood, this defines a <code class="docutils literal notranslate"><span class="pre">guide</span></code>
function with <code class="docutils literal notranslate"><span class="pre">Normal</span></code> distributions with learnable parameters
corresponding to each <code class="docutils literal notranslate"><span class="pre">sample()</span></code> in the model. Autoguide also supports
learning MAP estimates with <code class="docutils literal notranslate"><span class="pre">AutoDelta</span></code> or composing guides with
<code class="docutils literal notranslate"><span class="pre">AutoGuideList</span></code> (see the
<a class="reference external" href="http://docs.pyro.ai/en/dev/contrib.autoguide.html">docs</a> for more
information). In <a class="reference internal" href="bayesian_regression_ii.html"><span class="doc">Part II</span></a> we will
explore how to write guides by hand.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pyro.contrib.autoguide</span> <span class="k">import</span> <span class="n">AutoDiagonalNormal</span>
<span class="n">guide</span> <span class="o">=</span> <span class="n">AutoDiagonalNormal</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Inference">
<h2>Inference<a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h2>
<p>To do inference we’ll use stochastic variational inference (SVI) (for an
introduction to SVI, see <a class="reference internal" href="svi_part_i.html"><span class="doc">SVI Part I</span></a>). Just like
in the non-Bayesian linear regression, each iteration of our training
loop will take a gradient step, with the difference that in this case,
we’ll use the ELBO objective instead of the MSE loss by constructing a
<code class="docutils literal notranslate"><span class="pre">Trace_ELBO</span></code> object that we pass to <code class="docutils literal notranslate"><span class="pre">SVI</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optim</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">})</span>
<span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">guide</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">(),</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">Adam</span></code> is a thin wrapper around <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code> (see
<a class="reference internal" href="svi_part_i.html#Optimizers"><span class="std std-ref">here</span></a> for a discussion). To take an
ELBO gradient step we simply call the step method of SVI. Notice that
the data argument we pass to step will be passed to both model() and
guide(). The complete training loop is as follows:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="n">pyro</span><span class="o">.</span><span class="n">clear_param_store</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="c1"># calculate the loss and take a gradient step</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">svi</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[iteration </span><span class="si">%04d</span><span class="s2">] loss: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

<span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[iteration 0001] loss: 18.3907
[iteration 0101] loss: 3.1146
[iteration 0201] loss: 3.1067
[iteration 0301] loss: 2.8602
[iteration 0401] loss: 2.7770
[iteration 0501] loss: 2.6181
[iteration 0601] loss: 2.4298
[iteration 0701] loss: 2.0160
[iteration 0801] loss: 1.7814
[iteration 0901] loss: 1.5811
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">pyro</span><span class="o">.</span><span class="n">get_param_store</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">pyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
auto_loc tensor([-2.2026,  0.2936, -1.8873, -0.1607,  9.1753], requires_grad=True)
auto_scale tensor([0.2285, 0.0954, 0.1376, 0.0600, 0.1042], grad_fn=&lt;AddBackward0&gt;)
</pre></div></div>
</div>
<p>As you can see, instead of just point estimates, we now have uncertainty
estimates (<code class="docutils literal notranslate"><span class="pre">auto_scale</span></code>) for our learned parameters. Note that
Autoguide packs the latent variables into a tensor, in this case, one
entry per variable sampled in our model.</p>
</div>
<div class="section" id="Model-Evaluation">
<h2>Model Evaluation<a class="headerlink" href="#Model-Evaluation" title="Permalink to this headline">¶</a></h2>
<p>To evaluate our model, we’ll generate some predictive samples and look
at the posteriors. Since our variational distribution is fully
parameterized, we can just run the lifted model forward. We wrap the
model with a <code class="docutils literal notranslate"><span class="pre">Delta</span></code> distribution in order to register the values with
Pyro. We then store the execution traces in the <code class="docutils literal notranslate"><span class="pre">posterior</span></code> object
with <code class="docutils literal notranslate"><span class="pre">svi.run()</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">get_marginal</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">traces</span><span class="p">,</span> <span class="n">sites</span><span class="p">:</span><span class="n">EmpiricalMarginal</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">sites</span><span class="p">)</span><span class="o">.</span><span class="n">_get_samples_and_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">sites</span><span class="p">):</span>
    <span class="n">marginal</span> <span class="o">=</span> <span class="n">get_marginal</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">sites</span><span class="p">)</span>
    <span class="n">site_stats</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">marginal</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">site_name</span> <span class="o">=</span> <span class="n">sites</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">marginal_site</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">marginal</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
        <span class="n">describe</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="o">.</span><span class="n">describe</span><span class="p">,</span> <span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
        <span class="n">site_stats</span><span class="p">[</span><span class="n">site_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">marginal_site</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">describe</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> \
            <span class="p">[[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">,</span> <span class="s2">&quot;5%&quot;</span><span class="p">,</span> <span class="s2">&quot;25%&quot;</span><span class="p">,</span> <span class="s2">&quot;50%&quot;</span><span class="p">,</span> <span class="s2">&quot;75%&quot;</span><span class="p">,</span> <span class="s2">&quot;95%&quot;</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">site_stats</span>

<span class="k">def</span> <span class="nf">wrapped_model</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">):</span>
    <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">Delta</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)))</span>

<span class="n">posterior</span> <span class="o">=</span> <span class="n">svi</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># posterior predictive distribution we can get samples from</span>
<span class="n">trace_pred</span> <span class="o">=</span> <span class="n">TracePredictive</span><span class="p">(</span><span class="n">wrapped_model</span><span class="p">,</span>
                             <span class="n">posterior</span><span class="p">,</span>
                             <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">post_pred</span> <span class="o">=</span> <span class="n">trace_pred</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">post_summary</span> <span class="o">=</span> <span class="n">summary</span><span class="p">(</span><span class="n">post_pred</span><span class="p">,</span> <span class="n">sites</span><span class="o">=</span> <span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;obs&#39;</span><span class="p">])</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">post_summary</span><span class="p">[</span><span class="s2">&quot;prediction&quot;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">post_summary</span><span class="p">[</span><span class="s2">&quot;obs&quot;</span><span class="p">]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;cont_africa&quot;</span><span class="p">:</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;rugged&quot;</span><span class="p">:</span> <span class="n">x_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s2">&quot;mu_mean&quot;</span><span class="p">:</span> <span class="n">mu</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">],</span>
    <span class="s2">&quot;mu_perc_5&quot;</span><span class="p">:</span> <span class="n">mu</span><span class="p">[</span><span class="s2">&quot;5%&quot;</span><span class="p">],</span>
    <span class="s2">&quot;mu_perc_95&quot;</span><span class="p">:</span> <span class="n">mu</span><span class="p">[</span><span class="s2">&quot;95%&quot;</span><span class="p">],</span>
    <span class="s2">&quot;y_mean&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">],</span>
    <span class="s2">&quot;y_perc_5&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="s2">&quot;5%&quot;</span><span class="p">],</span>
    <span class="s2">&quot;y_perc_95&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="s2">&quot;95%&quot;</span><span class="p">],</span>
    <span class="s2">&quot;true_gdp&quot;</span><span class="p">:</span> <span class="n">y_data</span><span class="p">,</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">african_nations</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span><span class="p">[</span><span class="s2">&quot;cont_africa&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">non_african_nations</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span><span class="p">[</span><span class="s2">&quot;cont_africa&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">african_nations</span> <span class="o">=</span> <span class="n">african_nations</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">])</span>
<span class="n">non_african_nations</span> <span class="o">=</span> <span class="n">non_african_nations</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Regression line 90% CI&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;mu_mean&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
                   <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;mu_perc_5&quot;</span><span class="p">],</span>
                   <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;mu_perc_95&quot;</span><span class="p">],</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;true_gdp&quot;</span><span class="p">],</span>
           <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Terrain Ruggedness Index&quot;</span><span class="p">,</span>
          <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;log GDP (2000)&quot;</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Non African Nations&quot;</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;mu_mean&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
                   <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;mu_perc_5&quot;</span><span class="p">],</span>
                   <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;mu_perc_95&quot;</span><span class="p">],</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;true_gdp&quot;</span><span class="p">],</span>
           <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Terrain Ruggedness Index&quot;</span><span class="p">,</span>
          <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;log GDP (2000)&quot;</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s2">&quot;African Nations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[13]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[Text(0, 0.5, &#39;log GDP (2000)&#39;),
 Text(0.5, 0, &#39;Terrain Ruggedness Index&#39;),
 Text(0.5, 1.0, &#39;African Nations&#39;)]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bayesian_regression_25_1.png" src="_images/bayesian_regression_25_1.png" />
</div>
</div>
<p>The above figure shows the uncertainty in our estimate of the regression
line. Note that for lower values of ruggedness there are many more data
points, and as such, lesser wiggle room for the line of best fit. This
is reflected in the 90% CI around the mean. We can also see that most of
the data points actually lie outside the 90% CI, and this is expected
because we have not plotted the outcome variable which will be affected
by <code class="docutils literal notranslate"><span class="pre">sigma</span></code>! Let us do so next.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Posterior predictive distribution with 90% CI&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;y_mean&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
                   <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;y_perc_5&quot;</span><span class="p">],</span>
                   <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;y_perc_95&quot;</span><span class="p">],</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">non_african_nations</span><span class="p">[</span><span class="s2">&quot;true_gdp&quot;</span><span class="p">],</span>
           <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Terrain Ruggedness Index&quot;</span><span class="p">,</span>
          <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;log GDP (2000)&quot;</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Non African Nations&quot;</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">])</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;y_mean&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
                   <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;y_perc_5&quot;</span><span class="p">],</span>
                   <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;y_perc_95&quot;</span><span class="p">],</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;rugged&quot;</span><span class="p">],</span>
           <span class="n">african_nations</span><span class="p">[</span><span class="s2">&quot;true_gdp&quot;</span><span class="p">],</span>
           <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Terrain Ruggedness Index&quot;</span><span class="p">,</span>
          <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;log GDP (2000)&quot;</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s2">&quot;African Nations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[14]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[Text(0, 0.5, &#39;log GDP (2000)&#39;),
 Text(0.5, 0, &#39;Terrain Ruggedness Index&#39;),
 Text(0.5, 1.0, &#39;African Nations&#39;)]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bayesian_regression_27_1.png" src="_images/bayesian_regression_27_1.png" />
</div>
</div>
<p>We observe that the outcome from our model and the 90% CI accounts for
the majority of the data points that we observe in practice. It is
usually a good idea to do such posterior predictive checks to see if our
model gives valid predictions.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># we need to prepend `module$$$` to all parameters of nn.Modules since</span>
<span class="c1"># that is how they are stored in the ParamStore</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">get_marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;module$$$linear.weight&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">factor</span> <span class="o">=</span> <span class="n">get_marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;module$$$factor&#39;</span><span class="p">])</span>
<span class="n">gamma_within_africa</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">factor</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gamma_outside_africa</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">gamma_within_africa</span><span class="p">,</span> <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="s2">&quot;African nations&quot;</span><span class="p">},)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">gamma_outside_africa</span><span class="p">,</span> <span class="n">kde_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="s2">&quot;Non-African nations&quot;</span><span class="p">})</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Density of Slope : log(GDP) vs. Terrain Ruggedness&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[15]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>Text(0.5, 0.98, &#39;Density of Slope : log(GDP) vs. Terrain Ruggedness&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bayesian_regression_29_1.png" src="_images/bayesian_regression_29_1.png" />
</div>
</div>
<p>In the next section, we’ll look at how to write guides for variational
inference as well as compare the results with inference via HMC.</p>
<p>See an example with a toy dataset on
<a class="reference external" href="https://github.com/uber/pyro/blob/dev/examples/bayesian_regression.py">Github</a>.</p>
<div class="section" id="References">
<h3>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>McElreath, D., <em>Statistical Rethinking, Chapter 7</em>, 2016</li>
<li>Nunn, N. &amp; Puga, D., <a class="reference external" href="https://diegopuga.org/papers/rugged.pdf">Ruggedness: The blessing of bad geography in
Africa”</a>, Review of
Economics and Statistics 94(1), Feb.&nbsp;2012</li>
</ol>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bayesian_regression_ii.html" class="btn btn-neutral float-right" title="Bayesian Regression - Inference Algorithms (Part 2)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="vae.html" class="btn btn-neutral" title="Variational Autoencoders" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2018, Uber Technologies, Inc.

    </p>
  </div>

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.4.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>
