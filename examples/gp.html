

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gaussian Processes &mdash; Pyro Tutorials 0.2.0-a0+88254ac documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Pyro Tutorials 0.2.0-a0+88254ac documentation" href="index.html"/>
        <link rel="next" title="Bayesian Optimization" href="bo.html"/>
        <link rel="prev" title="Gaussian Mixture Model" href="gmm.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                pytorch-0.3-279-gdafa1ddd
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Models in Pyro: From Primitive Distributions to Stochastic Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Inference in Pyro: From Stochastic Functions to Marginal Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: An Introduction to Stochastic Variational Inference in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: Conditional Independence, Subsampling, and Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro 0.2</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vae.html">Variational Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">Bayesian Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">The Semi-Supervised VAE</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Imports">Imports</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Define-model">Define model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Inference">Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Fit-the-model-using-MAP">Fit the model using MAP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Sparse-GPs">Sparse GPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#More-Sparse-GPs">More Sparse GPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Gaussian Processes</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/gp.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Gaussian-Processes">
<h1>Gaussian Processes<a class="headerlink" href="#Gaussian-Processes" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian Processes</a>
have been used in supervised, unsupervised, and even reinforcement
learning problems and are described by an elegant mathematical theory
(for an overview of the subject see [1, 4]). They are also very
attractive conceptually, since they offer an intuitive way to define
priors over functions. And finally, since Gaussian Processes are
formulated in a Bayesian setting, they come equipped with a powerful
notion of uncertainty.</p>
<p>Happily, Pyro offers some support for Gaussian Processes in the
<code class="docutils literal"><span class="pre">pyro.contrib.gp</span></code> module. The goal of this tutorial is to give a brief
introduction to Gaussian Processes (GPs) in the context of this module.
We will mostly be focusing on how to use the GP interface in Pyro and
refer the reader to the references for more details about GPs in
general.</p>
<p>The model we’re interested in is defined by</p>
<div class="math">
\[f \sim \mathcal{GP}\left(0, \mathbf{K}_f(x, x')\right)\]</div>
<p>and</p>
<div class="math">
\[y = f(x) + \epsilon,\quad \epsilon \sim \mathcal{N}\left(0, \beta^{-1}\mathbf{I}\right).\]</div>
<p>Here <span class="math">\(x, x' \in\mathbf{X}\)</span> are points in the input space and
<span class="math">\(y\in\mathbf{Y}\)</span> is a point in the output space. <span class="math">\(f\)</span> is a
draw from the GP prior specified by the kernel <span class="math">\(\mathbf{K}_f\)</span> and
represents a function from <span class="math">\(\mathbf{X}\)</span> to <span class="math">\(\mathbf{Y}\)</span>.
Finally, <span class="math">\(\epsilon\)</span> represents Gaussian observation noise.</p>
<p>We will use the <a class="reference external" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">radial basis function
kernel</a>
(RBF kernel) as the kernel of our GP:</p>
<div class="math">
\[k(x,x') = \sigma^2 \exp\left(-\frac{\|x-x'\|^2}{2l^2}\right).\]</div>
<p>Here <span class="math">\(\sigma^2\)</span> and <span class="math">\(l\)</span> are parameters that specify the
kernel; specifically, <span class="math">\(\sigma^2\)</span> is a variance or amplitude
squared and <span class="math">\(l\)</span> is a lengthscale. We’ll get some intuition for
these parameters below.</p>
</div>
<div class="section" id="Imports">
<h2>Imports<a class="headerlink" href="#Imports" title="Permalink to this headline">¶</a></h2>
<p>First, we import necessary modules.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>from __future__ import print_function
import os
import matplotlib.pyplot as plt
import torch

import pyro
import pyro.contrib.gp as gp
import pyro.distributions as dist
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam

smoke_test = (&#39;CI&#39; in os.environ)  # ignore; used for checking code integrity in the Pyro repo
pyro.enable_validation(True)       # can help with debugging
pyro.set_rng_seed(0)
</pre></div>
</div>
</div>
<p>Throughout the tutorial we’ll want to visualize GPs. So we define a
helper function for plotting:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span># note that this helper function does three different things:
# (i) plots the observed data;
# (ii) plots the predictions from the learned GP after conditioning on data;
# (iii) plots samples from the GP prior (with no conditioning on observed data)

def plot(plot_observed_data=False, plot_predictions=False, n_prior_samples=0,
         model=None, kernel=None, n_test=500):

    plt.figure(figsize=(12, 6))
    if plot_observed_data:
        plt.plot(X.numpy(), y.numpy(), &#39;kx&#39;)
    if plot_predictions:
        Xtest = torch.linspace(-0.5, 5.5, n_test)  # test inputs
        # compute predictive mean and variance
        with torch.no_grad():
            if type(model) == gp.models.VariationalSparseGP:
                mean, cov = model(Xtest, full_cov=True)
            else:
                mean, cov = model(Xtest, full_cov=True, noiseless=False)
        sd = cov.diag().sqrt()  # standard deviation at each input point x
        plt.plot(Xtest.numpy(), mean.numpy(), &#39;r&#39;, lw=2)  # plot the mean
        plt.fill_between(Xtest.numpy(),  # plot the two-sigma uncertainty about the mean
                         (mean - 2.0 * sd).numpy(),
                         (mean + 2.0 * sd).numpy(),
                         color=&#39;C0&#39;, alpha=0.3)
    if n_prior_samples &gt; 0:  # plot samples from the GP prior
        Xtest = torch.linspace(-0.5, 5.5, n_test)  # test inputs
        noise = (model.noise if type(model) != gp.models.VariationalSparseGP
                 else model.likelihood.variance)
        cov = kernel.forward(Xtest) + noise.expand(n_test).diag()
        samples = dist.MultivariateNormal(torch.zeros(n_test), covariance_matrix=cov)\
                      .sample(sample_shape=(n_prior_samples,))
        plt.plot(Xtest.numpy(), samples.numpy().T, lw=2, alpha=0.4)

    plt.xlim(-0.5, 5.5)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Data">
<h2>Data<a class="headerlink" href="#Data" title="Permalink to this headline">¶</a></h2>
<p>The data consist of <span class="math">\(20\)</span> points sampled from</p>
<div class="math">
\[ \begin{align}\begin{aligned}y = 0.5\sin(3x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, 0.2).\\with :math:`x` sampled uniformly from the interval [0, 5].\end{aligned}\end{align} \]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>N = 20
X = dist.Uniform(0.0, 5.0).sample(sample_shape=(N,))
y = 0.5 * torch.sin(3*X) + dist.Normal(0.0, 0.2).sample(sample_shape=(N,))

plot(plot_observed_data=True)  # let&#39;s plot the observed data
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_8_0.png" src="_images/gp_8_0.png" />
</div>
</div>
</div>
<div class="section" id="Define-model">
<h2>Define model<a class="headerlink" href="#Define-model" title="Permalink to this headline">¶</a></h2>
<p>First we define a RBF kernel, specifying the values of the two
hyperparameters <code class="docutils literal"><span class="pre">variance</span></code> and <code class="docutils literal"><span class="pre">lengthscale</span></code>. Then we construct a
<code class="docutils literal"><span class="pre">GPRegression</span></code> object. Here we feed in another hyperparameter,
<code class="docutils literal"><span class="pre">noise</span></code>, that corresponds to <span class="math">\(\epsilon\)</span> above.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.), lengthscale=torch.tensor(10.))
gpr = gp.models.GPRegression(X, y, kernel, noise=torch.tensor(1.))
</pre></div>
</div>
</div>
<p>Let’s see what samples from this GP function prior look like. Note that
this is <em>before</em> we’ve conditioned on the data. The shape these
functions take—their smoothness, their vertical scale, etc.—is
controlled by the GP kernel.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>plot(model=gpr, kernel=kernel, n_prior_samples=2)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_12_0.png" src="_images/gp_12_0.png" />
</div>
</div>
<p>For example, if we make <code class="docutils literal"><span class="pre">variance</span></code> and <code class="docutils literal"><span class="pre">noise</span></code> smaller we will see
function samples with smaller vertical amplitude:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>kernel2 = gp.kernels.RBF(input_dim=1, variance=torch.tensor(0.1),
                         lengthscale=torch.tensor(10.))
gpr2 = gp.models.GPRegression(X, y, kernel2, noise=torch.tensor(0.1))
plot(model=gpr2, kernel=kernel2, n_prior_samples=2)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_14_0.png" src="_images/gp_14_0.png" />
</div>
</div>
</div>
<div class="section" id="Inference">
<h2>Inference<a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h2>
<p>In the above we set the kernel hyperparameters by hand. If we want to
learn the hyperparameters from the data, we need to do inference. In the
simplest (conjugate) case we do gradient ascent on the log marginal
likelihood. In <code class="docutils literal"><span class="pre">pyro.contrib.gp</span></code> we just use the Pyro <code class="docutils literal"><span class="pre">SVI</span></code>
inferface (even though we’re not sampling any latent random variables in
this case).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>optim = Adam({&quot;lr&quot;: 0.005})
svi = SVI(gpr.model, gpr.guide, optim, loss=Trace_ELBO())
losses = []
num_steps = 2500 if not smoke_test else 2
for i in range(num_steps):
    losses.append(svi.step())
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span># let&#39;s plot the loss curve after 2500 steps of training
plt.plot(losses);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_17_0.png" src="_images/gp_17_0.png" />
</div>
</div>
<p>Let’s see if we’re learned anything reasonable:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>plot(model=gpr, plot_observed_data=True, plot_predictions=True)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_19_0.png" src="_images/gp_19_0.png" />
</div>
</div>
<p>Here the thick red curve is the mean prediction and the blue band
represents the 2-sigma uncertainty around the mean. It seems we learned
reasonable kernel hyperparameters, as both the mean and uncertainty give
a reasonable fit to the data. (Note that learning could have easily gone
wrong if we e.g.&nbsp;chose too large of a learning rate or chose bad
initital hyperparameters.)</p>
<p>Note that the kernel is only well-defined if <code class="docutils literal"><span class="pre">variance</span></code> and
<code class="docutils literal"><span class="pre">lengthscale</span></code> are positive. Under the hood Pyro is using PyTorch
constraints (see
<a class="reference external" href="http://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraints">docs</a>)
to ensure that hyperparameters are constrained to the appropriate
domains. To see the constrained values of the hyperparameters, we will
use the <code class="docutils literal"><span class="pre">get_param(...)</span></code> method.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>gpr.kernel.get_param(&quot;variance&quot;).item()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[10]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.20291300117969513
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>gpr.kernel.get_param(&quot;lengthscale&quot;).item()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[11]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.5022150278091431
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>gpr.get_param(&quot;noise&quot;).item()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[12]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.04273035749793053
</pre></div>
</div>
</div>
<p>The period of the sinusoid that generated the data is
<span class="math">\(T = 2\pi/3 \approx 2.09\)</span> so learning a lengthscale that’s
approximiately equal to a quarter period makes sense.</p>
<div class="section" id="Fit-the-model-using-MAP">
<h3>Fit the model using MAP<a class="headerlink" href="#Fit-the-model-using-MAP" title="Permalink to this headline">¶</a></h3>
<p>We need to define priors for the hyperparameters.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span># note that our priors have support on the positive reals
gpr.kernel.set_prior(&quot;lengthscale&quot;, dist.LogNormal(0.0, 1.0))
gpr.kernel.set_prior(&quot;variance&quot;, dist.LogNormal(0.0, 1.0))
# we reset the param store so that the previous inference doesn&#39;t interfere with this one
pyro.clear_param_store()
optim = Adam({&quot;lr&quot;: 0.005})
svi = SVI(gpr.model, gpr.guide, optim, loss=Trace_ELBO())
losses = []
num_steps = 2500 if not smoke_test else 2
for i in range(num_steps):
    losses.append(svi.step())
plt.plot(losses);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_27_0.png" src="_images/gp_27_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>plot(model=gpr, plot_observed_data=True, plot_predictions=True)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_28_0.png" src="_images/gp_28_0.png" />
</div>
</div>
<p>Let’s inspect the hyperparameters we’ve learned:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>for param_name in pyro.get_param_store().get_all_param_names():
    print(&#39;{} = {}&#39;.format(param_name, pyro.param(param_name).item()))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
RBF$$$variance_MAP = 0.24471372365951538
GPR$$$noise = 0.04222385957837105
RBF$$$lengthscale_MAP = 0.5217667818069458
</pre></div></div>
</div>
<p>Note that the MAP values are different from the MLE values due to the
prior. We can also see that the names of parameters have been changed.
This is because by putting priors on the hyperparameters of our kernel,
we have transformed them into into random variables (they are no longer
<code class="docutils literal"><span class="pre">torch.nn.Parameter</span></code>s). Under the hood, the GP module appends a
trailing <code class="docutils literal"><span class="pre">_MAP</span></code> to each hyperparameter name to reflect this
difference.</p>
</div>
</div>
<div class="section" id="Sparse-GPs">
<h2>Sparse GPs<a class="headerlink" href="#Sparse-GPs" title="Permalink to this headline">¶</a></h2>
<p>For large datasets computing the log marginal likelihood is costly due
to the expensive matrix operations involved (e.g.&nbsp;see Section 2.2 of
[1]). A variety of so-called ‘sparse’ variational methods have been
developed to make GPs viable for larger datasets. This is a big area of
research and we won’t be going into all the details. Instead we quickly
show how we can use <code class="docutils literal"><span class="pre">SparseGPRegression</span></code> in <code class="docutils literal"><span class="pre">pyro.contrib.gp</span></code> to
make use of these methods.</p>
<p>First, we generate more data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>N = 1000
X = dist.Uniform(0.0, 5.0).sample(sample_shape=(N,))
y = 0.5 * torch.sin(3*X) + dist.Normal(0.0, 0.2).sample(sample_shape=(N,))
plot(plot_observed_data=True)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_33_0.png" src="_images/gp_33_0.png" />
</div>
</div>
<p>Using the sparse GP is very similar to using the basic GP used above. We
just need to add an extra parameter <span class="math">\(X_u\)</span> (the inducing points).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span># initialize the inducing inputs
Xu = torch.arange(20) / 4.0

# initialize the kernel and model
kernel = gp.kernels.RBF(input_dim=1)
# we increase the jitter for better numerical stability
sgpr = gp.models.SparseGPRegression(X, y, kernel, Xu=Xu, jitter=1.0e-5)

# the way we setup inference is similar to above
pyro.clear_param_store()
optim = Adam({&quot;lr&quot;: 0.005})
svi = SVI(sgpr.model, sgpr.guide, optim, loss=Trace_ELBO())
losses = []
num_steps = 2500 if not smoke_test else 2
for i in range(num_steps):
    losses.append(svi.step())
plt.plot(losses);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_35_0.png" src="_images/gp_35_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span># let&#39;s look at the inducing points we&#39;ve learned
print(&quot;inducing points:\n{}&quot;.format(pyro.param(&quot;SGPR$$$Xu&quot;).data.numpy()))
# and plot the predictions from the sparse GP
plot(model=sgpr, plot_observed_data=True, plot_predictions=True)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
inducing points:
[0.0295011  0.24607515 0.44147182 0.7126421  1.0303956  1.3245093
 1.5803136  1.877679   2.1162455  2.4676545  2.4189417  2.8084621
 3.070498   3.3058653  3.577411   3.8880596  4.1536465  4.4360414
 4.709121   4.9547815 ]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_36_1.png" src="_images/gp_36_1.png" />
</div>
</div>
<p>We can see that the model learns a reasonable fit to the data. There are
three different sparse approximations that are currently implemented in
Pyro: - “DTC” (Deterministic Training Conditional) - “FITC” (Partially
Independent Training Conditional) - “VFE” (Variational Free Energy)</p>
<p>By default, <code class="docutils literal"><span class="pre">SparseGPRegression</span></code> will use “VFE” as the inference
method. We can use other methods by passing a different <code class="docutils literal"><span class="pre">approx</span></code> flag
to <code class="docutils literal"><span class="pre">SparseGPRegression</span></code>.</p>
</div>
<div class="section" id="More-Sparse-GPs">
<h2>More Sparse GPs<a class="headerlink" href="#More-Sparse-GPs" title="Permalink to this headline">¶</a></h2>
<p>Both <code class="docutils literal"><span class="pre">GPRegression</span></code> and <code class="docutils literal"><span class="pre">SparseGPRegression</span></code> above are limited to
Gaussian likelihoods. We can use other likelihoods with GPs—for example,
we can use the Bernoulli likelihood for classification problems—but the
inference problem becomes more difficult. In this section, we show how
to use the <code class="docutils literal"><span class="pre">VariationalSparseGP</span></code> module, which can handle non-Gaussian
likelihoods. So we can compare to what we’ve done above, we’re still
going to use a Gaussian likelihood. The point is that the inference
that’s being done under the hood can support other likelihoods.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span># initialize the inducing inputs
Xu = torch.arange(10) / 2.0

# initialize the kernel, likelihood, and model
kernel = gp.kernels.RBF(input_dim=1)
likelihood = gp.likelihoods.Gaussian()
# turn on &quot;whiten&quot; flag for more stable optimization
vsgp = gp.models.VariationalSparseGP(X, y, kernel, Xu=Xu, likelihood=likelihood, whiten=True)

pyro.clear_param_store()
# instead of defining our own training loop, we will
# use the built-in support provided by the GP module
num_steps = 1500 if not smoke_test else 2
losses = vsgp.optimize(optimizer=Adam({&quot;lr&quot;: 0.01}), num_steps=num_steps)
plt.plot(losses);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_40_0.png" src="_images/gp_40_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="input_area highlight-ipython2"><div class="highlight"><pre>
<span></span>plot(model=vsgp, plot_observed_data=True, plot_predictions=True)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/gp_41_0.png" src="_images/gp_41_0.png" />
</div>
</div>
<p>That’s all there is to it. For more details on the <code class="docutils literal"><span class="pre">pyro.contrib.gp</span></code>
module see the <a class="reference external" href="http://docs.pyro.ai/en/dev/contrib/gp.html">docs</a>.
And for example code that uses a GP for classification see
<a class="reference external" href="https://github.com/uber/pyro/blob/dev/examples/contrib/gp/sv-dkl.py">here</a>.</p>
</div>
<div class="section" id="Reference">
<h2>Reference<a class="headerlink" href="#Reference" title="Permalink to this headline">¶</a></h2>
<p>[1]
<code class="docutils literal"><span class="pre">Deep</span> <span class="pre">Gaussian</span> <span class="pre">processes</span> <span class="pre">and</span> <span class="pre">variational</span> <span class="pre">propagation</span> <span class="pre">of</span> <span class="pre">uncertainty</span></code>,
Andreas Damianou</p>
<p>[2]
<code class="docutils literal"><span class="pre">A</span> <span class="pre">unifying</span> <span class="pre">framework</span> <span class="pre">for</span> <span class="pre">sparse</span> <span class="pre">Gaussian</span> <span class="pre">process</span> <span class="pre">approximation</span> <span class="pre">using</span> <span class="pre">power</span> <span class="pre">expectation</span> <span class="pre">propagation</span></code>,
Thang D. Bui, Josiah Yan, and Richard E. Turner</p>
<p>[3] <code class="docutils literal"><span class="pre">Scalable</span> <span class="pre">variational</span> <span class="pre">Gaussian</span> <span class="pre">process</span> <span class="pre">classification</span></code>,&nbsp;&nbsp;&nbsp;&nbsp; James
Hensman, Alexander G. de G. Matthews, and Zoubin Ghahramani</p>
<p>[4] <code class="docutils literal"><span class="pre">Gaussian</span> <span class="pre">Processes</span> <span class="pre">for</span> <span class="pre">Machine</span> <span class="pre">Learning</span></code>,&nbsp;&nbsp;&nbsp;&nbsp; Carl E. Rasmussen,
and Christopher K. I. Williams</p>
<p>[5]
<code class="docutils literal"><span class="pre">A</span> <span class="pre">Unifying</span> <span class="pre">View</span> <span class="pre">of</span> <span class="pre">Sparse</span> <span class="pre">Approximate</span> <span class="pre">Gaussian</span> <span class="pre">Process</span> <span class="pre">Regression</span></code>,
Joaquin Quinonero-Candela, and Carl E. Rasmussen</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bo.html" class="btn btn-neutral float-right" title="Bayesian Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gmm.html" class="btn btn-neutral" title="Gaussian Mixture Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2018, Uber Technologies, Inc.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2.0-a0+88254ac',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>